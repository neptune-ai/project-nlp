{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64620f15",
   "metadata": {},
   "source": [
    "# Text classification using fastText and Optuna with Neptune tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a9bda",
   "metadata": {},
   "source": [
    "## [Neptune] Install the neptune-notebooks widget\n",
    "The neptune-notebooks jupyter extension lets you version, manage and share notebook checkpoints in your projects, without leaving your notebook.  \n",
    "[Read the docs](https://docs.neptune.ai/integrations-and-supported-tools/ide-and-notebooks/jupyter-lab-and-jupyter-notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c6c96c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354eb143",
   "metadata": {},
   "source": [
    "### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab73da67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66d6d1a",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdf592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import fasttext\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "path = Path()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee8750",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba35018",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_JOBS = 4\n",
    "N_TRIALS = 10\n",
    "UPLOAD_SIZE_THRESHOLD = 400  # in MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df3020",
   "metadata": {},
   "source": [
    "### Load input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff81431",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH_S3 = \"s3://neptune-examples/data/text-classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e76274",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(f\"{DATASET_PATH_S3}/legal_text_classification.csv\")\n",
    "df_raw.dropna(subset=[\"case_text\"], inplace=True)\n",
    "df_raw.drop_duplicates(subset=\"case_text\", inplace=True)\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea33112",
   "metadata": {},
   "source": [
    "## [Neptune] Initialize a neptune project\n",
    "A project is a collection of runs, models, and other metadata created by project members. Typically you should create one project per machine learning task, to make it easy to compare runs that are connected to building certain kinds of ML model.  \n",
    "[Read the docs](https://docs.neptune.ai/you-should-know/core-concepts#project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b74ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune.new as neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_NAME = \"showcase\"\n",
    "PROJECT_NAME = \"project-text-classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e9d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = neptune.init_project(name=f\"{WORKSPACE_NAME}/{PROJECT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71978c10",
   "metadata": {},
   "source": [
    "## [Neptune] Log project level metadata\n",
    "All metadata common across all runs in a project (for example - input and configuration files) should be logged at the project level itself for easier management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363db1ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [Neptune] Version and track datasets\n",
    "Neptune lets you track pinters to datasets, models, and other artifacts stored locally or in S3.  \n",
    "[Read the docs](https://docs.neptune.ai/how-to-guides/data-versioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c678e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "project[\"data/files\"].track_files(str(DATASET_PATH_S3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e36a45",
   "metadata": {},
   "source": [
    "### [Neptune] Log dataset sample\n",
    "Smaller artifacts can also be uploaded directly to Neptune.  \n",
    "[Read the docs](https://docs.neptune.ai/you-should-know/what-can-you-log-and-display#files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1989664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune.new.types import File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b701a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_buffer = StringIO()\n",
    "df_raw.sample(100).to_csv(csv_buffer, index=False)\n",
    "project[\"data/sample\"].upload(File.from_stream(csv_buffer, extension=\"csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f1b070",
   "metadata": {},
   "source": [
    "### [Neptune] Log metadata plots\n",
    "Similar to other articats, you can also upload images and plot objects to Neptune.  \n",
    "[Read the docs](https://docs.neptune.ai/you-should-know/what-can-you-log-and-display#images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcddcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df_raw.case_outcome.value_counts().plot(kind=\"bar\")\n",
    "fig.update_xaxes(title=\"Case outcome\")\n",
    "fig.update_yaxes(title=\"No. of cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a75626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project[\"data/distribution\"].upload(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dea50b",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3cac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    \"\"\"Cleans a dataframe `df` string column `col` by applying the following transformations:\n",
    "    * Convert string to lower-case\n",
    "    * Remove punctuation\n",
    "    * Remove numbers\n",
    "    * Remove single-letter words\n",
    "    * Remove stopwords\n",
    "    * Remove multiple and leading/trailing whitespaces\n",
    "\n",
    "    Args:\n",
    "        df: Dataframe containing sgtring columns `col` to be cleaned\n",
    "        col: String column to be cleaned\n",
    "\n",
    "    Returns:\n",
    "        A copy of the dataframe `df` with the column `col` cleaned\n",
    "    \"\"\"\n",
    "\n",
    "    tqdm.pandas()\n",
    "    stop = set(stopwords.words(\"english\"))\n",
    "    pat = r\"\\b(?:{})\\b\".format(\"|\".join(stop))\n",
    "\n",
    "    _df = df.copy()\n",
    "    _df[col] = (\n",
    "        df[col]\n",
    "        .progress_apply(str.lower)  # Converting to lowercase\n",
    "        .progress_apply(lambda x: re.sub(r\"[^\\w\\s]\", \" \", x))  # Removing punctuation\n",
    "        .progress_apply(lambda x: \" \".join(x for x in x.split() if not any(c.isdigit() for c in x)))  # Removing numbers\n",
    "        .progress_apply(lambda x: re.sub(r\"\\b\\w\\b\", \"\", x))  # Removing single-letter words\n",
    "        .str.replace(pat, \"\", regex=True)  # Removing stopwords\n",
    "        .progress_apply(lambda x: re.sub(r\" +\", \" \", x))  # Removing multiple-whitespaces\n",
    "        .str.strip()  # Removing leading and whitepaces\n",
    "    )\n",
    "\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0812d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fasttext_raw = df_raw[[\"case_outcome\", \"case_text\"]]\n",
    "df_fasttext_raw[\"label\"] = \"__label__\" + df_fasttext_raw.case_outcome.str.replace(\" \", \"_\")\n",
    "df_fasttext_raw = df_fasttext_raw[[\"label\", \"case_text\"]]\n",
    "df_fasttext_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa0d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH_LOCAL = path.cwd().parent.parent.joinpath(\"data\")\n",
    "\n",
    "if not os.path.exists(DATASET_PATH_LOCAL):\n",
    "    os.makedirs(DATASET_PATH_LOCAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a053f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH_LOCAL_FASTTEXT = DATASET_PATH_LOCAL.joinpath(\"fasttext\")\n",
    "\n",
    "if not os.path.exists(DATASET_PATH_LOCAL_FASTTEXT):\n",
    "    os.makedirs(DATASET_PATH_LOCAL_FASTTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42790cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_CSV_KWARGS = {\n",
    "    \"sep\": \" \",\n",
    "    \"header\": False,\n",
    "    \"index\": False,\n",
    "    \"quoting\": csv.QUOTE_NONE,\n",
    "    \"quotechar\": \"\",\n",
    "    \"escapechar\": \" \",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1ab6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fasttext_raw.to_csv(DATASET_PATH_LOCAL_FASTTEXT.joinpath(\"raw.txt\"), **TO_CSV_KWARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42370733",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -5 $DATASET_PATH_LOCAL_FASTTEXT\"/raw.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d5cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = clean_text(df_fasttext_raw, \"case_text\")\n",
    "df_processed.drop_duplicates(subset=\"case_text\", inplace=True)\n",
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1510c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.to_csv(DATASET_PATH_LOCAL_FASTTEXT.joinpath(\"processed.txt\"), **TO_CSV_KWARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d185e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -5 $DATASET_PATH_LOCAL_FASTTEXT\"/processed.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_processed[\"case_text\"]\n",
    "y = df_processed[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a9218",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_, y_train, y_ = train_test_split(X, y, stratify=y, train_size=0.7)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_, y_, stratify=y_, train_size=0.5)\n",
    "\n",
    "print(f\"Training size: {X_train.shape}\")\n",
    "print(f\"Validation size: {X_valid.shape}\")\n",
    "print(f\"Test size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3578dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(data=[y_train, X_train]).T\n",
    "df_valid = pd.DataFrame(data=[y_valid, X_valid]).T\n",
    "df_test = pd.DataFrame(data=[y_test, X_test]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c41f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(DATASET_PATH_LOCAL_FASTTEXT.joinpath(\"train.txt\"), **TO_CSV_KWARGS)\n",
    "df_valid.to_csv(DATASET_PATH_LOCAL_FASTTEXT.joinpath(\"valid.txt\"), **TO_CSV_KWARGS)\n",
    "df_test.to_csv(DATASET_PATH_LOCAL_FASTTEXT.joinpath(\"test.txt\"), **TO_CSV_KWARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6166a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## [Neptune] Initialize an optuna study-level run\n",
    "A run is a namespace inside a project where you log metadata. Typically, you create a run every time you execute a script that does model training, re-training, or inference.  \n",
    "[Read the docs](https://docs.neptune.ai/you-should-know/core-concepts#run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954c8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = neptune.init_run(\n",
    "    project=f\"{WORKSPACE_NAME}/{PROJECT_NAME}\",\n",
    "    name=\"Fasttext text classification\",\n",
    "    description=\"Optuna tuned fasttext text classification\",\n",
    "    tags=[\"fasttext\", \"optuna\", \"study-level\", \"notebook\", \"sagemaker\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be504a1b",
   "metadata": {},
   "source": [
    "### [Neptune] Track run-specific files\n",
    "These are the files which are created during the run, and should be tracked within the run and not the project.  \n",
    "[Read the docs](https://docs.neptune.ai/how-to-guides/data-versioning/compare-datasets#step-2-add-tracking-of-the-dataset-version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3886e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"data/files\"].track_files(os.path.relpath(DATASET_PATH_LOCAL_FASTTEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3a4ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_buffer = StringIO()\n",
    "\n",
    "df_fasttext_raw.sample(100).to_csv(csv_buffer, index=False)\n",
    "run[\"data/sample\"].upload(File.from_stream(csv_buffer, extension=\"csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd68d365",
   "metadata": {},
   "source": [
    "### [Neptune] Log metadata to run\n",
    "You can log nested dictionaries to create custom nested namespaces.  \n",
    "[Read the docs](https://docs.neptune.ai/you-should-know/logging-metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad570b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"train_size\": len(df_train),\n",
    "    \"test_size\": len(df_test),\n",
    "}\n",
    "\n",
    "run[\"data/metadata\"] = metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a510c867",
   "metadata": {},
   "source": [
    "### [Neptune] Log sweep and trial parameters\n",
    "Neptune's Optuna integration lets you log metadata from both the study-level and trial-level runs.  \n",
    "[Read the docs](https://docs.neptune.ai/integrations-and-supported-tools/hyperparameter-optimization/optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba07f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "sweep_id = uuid.uuid1()\n",
    "print(f\"Optuna sweep-id: {sweep_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba224bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"study/sweep_id\"] = sweep_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c857de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_with_logging(trial: optuna.trial.Trial) -> int:\n",
    "    \"\"\"Optuna objective function with inbuilt Neptune tracking\n",
    "\n",
    "    Args:\n",
    "        trial (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"lr\": trial.suggest_float(\"lr\", 0.1, 1, step=0.1),\n",
    "        \"dim\": trial.suggest_int(\"dim\", 10, 1000, log=True),\n",
    "        \"ws\": trial.suggest_int(\"ws\", 1, 10),\n",
    "        \"epoch\": trial.suggest_int(\"epoch\", 10, 100),\n",
    "        \"minCount\": trial.suggest_int(\"minCount\", 1, 10),\n",
    "        \"wordNgrams\": trial.suggest_int(\"wordNgrams\", 1, 3),\n",
    "        \"loss\": trial.suggest_categorical(\"loss\", [\"hs\", \"softmax\", \"ova\"]),\n",
    "        \"bucket\": trial.suggest_int(\"bucket\", 1000000, 3000000, log=True),\n",
    "        \"lrUpdateRate\": trial.suggest_int(\"lrUpdateRate\", 1, 10),\n",
    "        \"t\": trial.suggest_float(\"t\", 0.00001, 0.1, log=True),\n",
    "    }\n",
    "\n",
    "    # (neptune) create a trial-level Run\n",
    "    run_trial_level = neptune.init_run(\n",
    "        project=f\"{WORKSPACE_NAME}/{PROJECT_NAME}\",\n",
    "        name=\"Fasttext text classification\",\n",
    "        description=\"Optuna tuned fasttext text classification\",\n",
    "        tags=[\"fasttext\", \"optuna\", \"trial-level\", \"notebook\", \"sagemaker\"],\n",
    "    )\n",
    "\n",
    "    # (neptune) log sweep id to trial-level Run\n",
    "    run_trial_level[\"sweep_id\"] = sweep_id\n",
    "\n",
    "    # (neptune) log parameters of a trial-level Run\n",
    "    clf = fasttext.train_supervised(\n",
    "        input=str(DATASET_PATH_LOCAL_FASTTEXT.joinpath(\"train.txt\")),\n",
    "        verbose=0,\n",
    "        **params,\n",
    "    )\n",
    "\n",
    "    properties = {k: v for k, v in vars(clf).items() if k not in [\"_words\", \"f\"]}\n",
    "    run_trial_level[\"model/properties\"] = properties\n",
    "\n",
    "    # (neptune) run training and calculate the score for this parameter configuration\n",
    "    preds = [clf.predict(text)[0][0] for text in X_valid.values]\n",
    "\n",
    "    run_trial_level[\"validation/metrics/classification_report\"] = classification_report(\n",
    "        y_valid,\n",
    "        preds,\n",
    "        output_dict=True,\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
    "        y_valid,\n",
    "        preds,\n",
    "        average=\"weighted\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    run_trial_level[\"validation/metrics/precision\"] = precision\n",
    "    run_trial_level[\"validation/metrics/recall\"] = recall\n",
    "    run_trial_level[\"validation/metrics/f1_score\"] = f1_score\n",
    "\n",
    "    # (neptune) stop trial-level Run\n",
    "    run_trial_level.stop()\n",
    "\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceec484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune.new.integrations.optuna as optuna_utils\n",
    "\n",
    "neptune_callback = optuna_utils.NeptuneCallback(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6775eda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(\n",
    "    objective_with_logging,\n",
    "    n_trials=N_TRIALS,\n",
    "    callbacks=[neptune_callback],\n",
    "    n_jobs=N_JOBS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4263fa7f",
   "metadata": {},
   "source": [
    "### [Neptune] Register a model \n",
    "With Neptune's model registry, you can store your ML models in a central location and collaboratively manage their lifecycle.  \n",
    "[Read the docs](https://docs.neptune.ai/how-to-guides/model-registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7614ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neptune.init_model(\n",
    "    model=\"TXTCLF-FTXT\",  # Reinitializing an existing model\n",
    "    # name=\"fasttext\", # Required only for new models\n",
    "    # key=\"FTXT\", # Required only for new models\n",
    "    project=f\"{WORKSPACE_NAME}/{PROJECT_NAME}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23dfbf8",
   "metadata": {},
   "source": [
    "#### [Neptune] Create a new model version\n",
    "For each model, you can create different versions as you refine the model.  \n",
    "[Read the docs](https://docs.neptune.ai/how-to-guides/model-registry/creating-model-versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a048095",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = neptune.init_model_version(\n",
    "    project=f\"{WORKSPACE_NAME}/{PROJECT_NAME}\",\n",
    "    model=model.get_structure()[\"sys\"][\"id\"].fetch(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ef38a8",
   "metadata": {},
   "source": [
    "#### [Neptune] Associate model version to run and vice-versa\n",
    "This is to help find the model created by the run in the runs table, and the run which created the model in the models table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4d5fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dict = {\n",
    "    \"id\": run.get_structure()[\"sys\"][\"id\"].fetch(),\n",
    "    \"name\": run.get_structure()[\"sys\"][\"name\"].fetch(),\n",
    "    \"url\": run.get_run_url(),\n",
    "}\n",
    "run_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5688b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version[\"run\"] = run_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12224677",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version_dict = {\n",
    "    \"id\": model_version.get_structure()[\"sys\"][\"id\"].fetch(),\n",
    "    \"url\": model_version.get_url(),\n",
    "}\n",
    "model_version_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"model\"] = model_version_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ccef04",
   "metadata": {},
   "source": [
    "#### Train model based on Hyperparameters chosen by Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f332ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = fasttext.train_supervised(\n",
    "    input=str(DATASET_PATH_LOCAL_FASTTEXT.joinpath(\"train.txt\")),\n",
    "    verbose=5,\n",
    "    **study.best_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f47ba4",
   "metadata": {},
   "source": [
    "#### [Neptune] Upload serialized model to model registry\n",
    "Similar to artifact tracking in a project/run, you can also track a pointer to the model in the model registry, or upload the entire serialized model object as well.  \n",
    "[Read the docs](https://docs.neptune.ai/how-to-guides/model-registry/creating-model-versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf14f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = path.cwd().parent.parent.joinpath(\"models\")\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258e3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "MODEL_NAME = str(MODEL_PATH.joinpath(f\"fasttext_{datetime.now().strftime('%Y%m%d%H%M%S')}.bin\"))\n",
    "clf.save_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ea5d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.getsize(MODEL_NAME) < 1024 * 1024 * UPLOAD_SIZE_THRESHOLD:  # 100 MB\n",
    "    print(\"Uploading serialized model\")\n",
    "    model_version[\"serialized_model\"].upload(str(MODEL_NAME))\n",
    "else:\n",
    "    print(f\"Model is larger than UPLOAD_SIZE_THRESHOLD ({UPLOAD_SIZE_THRESHOLD} MB). Tracking pointer to model file\")\n",
    "    model_version[\"serialized_model\"].track_files(os.path.relpath(MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5537ff3e",
   "metadata": {},
   "source": [
    "#### [Neptune] Log model properties to model_version\n",
    "Neptune dynamically creates nested namespaces based on the dictionary structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec647ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = {k: v for k, v in vars(clf).items() if k not in [\"_words\", \"f\"]}\n",
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b6f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version[\"properties\"] = properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab98241",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da49e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [clf.predict(text)[0][0] for text in X_test.values]\n",
    "set(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"prediction\"] = preds\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5024458",
   "metadata": {},
   "source": [
    "### [Neptune] Log parameters, metrics and debugging information to run and model version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a852cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
    "    y_test,\n",
    "    preds,\n",
    "    average=\"weighted\",\n",
    "    zero_division=0,\n",
    ")\n",
    "\n",
    "print(f\"Precision: {precision}\\nRecall: {recall}\\nF1-score: {f1_score}\")\n",
    "\n",
    "run[\"test/metrics/precision\"] = model_version[\"metrics/precision\"] = precision\n",
    "run[\"test/metrics/recall\"] = model_version[\"metrics/recall\"] = recall\n",
    "run[\"test/metrics/f1_score\"] = model_version[\"metrics/f1_score\"] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae87f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds, zero_division=0))\n",
    "\n",
    "# (neptune) Log each metric in its separate nested namespace\n",
    "run[\"test/metrics/classification_report\"] = classification_report(y_test, preds, output_dict=True, zero_division=0)\n",
    "\n",
    "# (neptune) Log classification report as an HTML dataframe\n",
    "df_clf_rpt = pd.DataFrame(classification_report(y_test, preds, output_dict=True, zero_division=0)).T\n",
    "run[\"test/metrics/classification_report/report\"].upload(File.as_html(df_clf_rpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f4a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ConfusionMatrixDisplay.from_predictions(y_test, preds, xticks_rotation=\"vertical\", colorbar=False)\n",
    "run[\"test/debug/plots/confusion_matrix\"].upload(fig.figure_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e72d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [s.replace(\"__label__\", \"\") for s in df_test.label.value_counts().index]\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Bar(name=\"Actual\", x=labels, y=df_test.label.value_counts()),\n",
    "        go.Bar(name=\"Prediction\", x=labels, y=df_test.prediction.value_counts()),\n",
    "    ]\n",
    ")\n",
    "fig.update_layout(title=\"Actual vs Prediction\", barmode=\"group\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585d991",
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"test/debug/plots/prediction_distribution\"].upload(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4530944b",
   "metadata": {},
   "source": [
    "### [Neptune] Log misclassified results\n",
    "CSV files logged to Neptune will be rendered as an interactive table.  \n",
    "[Read the docs](https://docs.neptune.ai/you-should-know/what-can-you-log-and-display#csv-files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b0aba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_debug = df_test[df_test.label != df_test.prediction]\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "\n",
    "df_debug.to_csv(csv_buffer, index=False)\n",
    "run[\"test/debug/misclassifications\"].upload(File.from_stream(csv_buffer, extension=\"csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f6740",
   "metadata": {},
   "source": [
    "### [Neptune] Stop model version, model, run, and project\n",
    "When a script finishes executing, runs are stopped automatically. However, to avoid logging metadata for longer than intended, it's best to stop the run explicitly with the `stop()` method.  \n",
    "When working in an interactive notebook environment, you should always call `stop()`. This is because the connection to Neptune is not stopped when the cell has finished executing, but rather only when you stop the notebook.  \n",
    "[Read the docs](https://docs.neptune.ai/api-reference/project#.stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd169a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version.stop()\n",
    "model.stop()\n",
    "run.stop()\n",
    "project.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc2b347",
   "metadata": {},
   "source": [
    "## [Neptune] Explore the [project](https://app.neptune.ai/showcase/project-text-classification) in the Neptune app\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "neptune": {
   "notebookId": "9732b722-0ff0-4d32-87ae-51ffcc75ab45",
   "projectVersion": 2
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "vscode": {
   "interpreter": {
    "hash": "ec001e92a202bf5caf822e845ea5ad1be0f2e89477ccf084fef05c1d70e5b02a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
