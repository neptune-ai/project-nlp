{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a99df7b3",
   "metadata": {},
   "source": [
    "# Text classification using fastText and Optuna with Neptune tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9c7524",
   "metadata": {},
   "source": [
    "## (Neptune) Install the neptune-notebooks widget\n",
    "The neptune-notebooks jupyter extension lets you version, manage and share notebook checkpoints in your projects, without leaving your notebook.  \n",
    "[Read the docs](https://docs.neptune.ai/integrations-and-supported-tools/ide-and-notebooks/jupyter-lab-and-jupyter-notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3661291",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a58b5c9",
   "metadata": {},
   "source": [
    "### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e499d77a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd8a6cd",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a3c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import fasttext\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "path = Path()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a9df2",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_JOBS = 4\n",
    "N_TRIALS = 10\n",
    "UPLOAD_SIZE_THRESHOLD = 500  # in MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5226c008",
   "metadata": {},
   "source": [
    "### Load input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1afde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH_S3 = \"s3://neptune-examples/data/text-classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb6ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(f\"{DATASET_PATH_S3}/legal_text_classification.csv\")\n",
    "df_raw.dropna(subset=[\"case_text\"], inplace=True)\n",
    "df_raw.drop_duplicates(subset=\"case_text\", inplace=True)\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b9b2d5",
   "metadata": {},
   "source": [
    "## (Neptune) Initialize a neptune project\n",
    "**A project is a collection of runs, models, and other metadata created by project members.** Typically you should create one project per machine learning task, to make it easy to compare runs that are connected to building certain kinds of ML model.  \n",
    "[Read the docs](https://docs.neptune.ai/you-should-know/core-concepts#project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d2ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune.new as neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0096701",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_NAME = \"showcase\"\n",
    "PROJECT_NAME = \"project-text-classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = neptune.init_project(name=f\"{WORKSPACE_NAME}/{PROJECT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dce852",
   "metadata": {},
   "source": [
    "## (Neptune) Log project level metadata\n",
    "All metadata common across all runs in a project (for example - input and configuration files) should be logged at the project level itself for easier management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adee996",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (Neptune) Version and track datasets\n",
    "Neptune lets you track pointers to datasets, models, and other artifacts stored locally or in S3.  \n",
    "[Read the docs](https://docs.neptune.ai/how-to-guides/data-versioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc3099",
   "metadata": {},
   "outputs": [],
   "source": [
    "project[\"data/files\"].track_files(str(DATASET_PATH_S3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b2b163",
   "metadata": {},
   "source": [
    "### (Neptune) Log dataset sample\n",
    "Smaller artifacts can also be uploaded directly to Neptune.  \n",
    "[Read the docs](https://docs.neptune.ai/you-should-know/what-can-you-log-and-display#files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ea387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune.new.types import File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d56dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_buffer = StringIO()\n",
    "df_raw.sample(100).to_csv(csv_buffer, index=False)\n",
    "project[\"data/sample\"].upload(File.from_stream(csv_buffer, extension=\"csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed026187",
   "metadata": {},
   "source": [
    "### (Neptune) Log metadata plots\n",
    "Similar to other articats, you can also upload images and plot objects to Neptune.  \n",
    "[Read the docs](https://docs.neptune.ai/you-should-know/what-can-you-log-and-display#images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a7616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df_raw.case_outcome.value_counts().plot(kind=\"bar\")\n",
    "fig.update_xaxes(title=\"Case outcome\")\n",
    "fig.update_yaxes(title=\"No. of cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a6fe15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project[\"data/distribution\"].upload(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a8abef",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda0638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    \"\"\"Cleans a dataframe `df` string column `col` by applying the following transformations:\n",
    "    * Convert string to lower-case\n",
    "    * Remove punctuation\n",
    "    * Remove numbers\n",
    "    * Remove single-letter words\n",
    "    * Remove stopwords\n",
    "    * Remove multiple and leading/trailing whitespaces\n",
    "\n",
    "    Args:\n",
    "        df: Dataframe containing sgtring columns `col` to be cleaned\n",
    "        col: String column to be cleaned\n",
    "\n",
    "    Returns:\n",
    "        A copy of the dataframe `df` with the column `col` cleaned\n",
    "    \"\"\"\n",
    "\n",
    "    tqdm.pandas()\n",
    "    stop = set(stopwords.words(\"english\"))\n",
    "    pat = r\"\\b(?:{})\\b\".format(\"|\".join(stop))\n",
    "\n",
    "    _df = df.copy()\n",
    "    _df[col] = (\n",
    "        df[col]\n",
    "        .progress_apply(str.lower)  # Converting to lowercase\n",
    "        .progress_apply(lambda x: re.sub(r\"[^\\w\\s]\", \" \", x))  # Removing punctuation\n",
    "        .progress_apply(lambda x: \" \".join(x for x in x.split() if not any(c.isdigit() for c in x)))  # Removing numbers\n",
    "        .progress_apply(lambda x: re.sub(r\"\\b\\w\\b\", \"\", x))  # Removing single-letter words\n",
    "        .str.replace(pat, \"\", regex=True)  # Removing stopwords\n",
    "        .progress_apply(lambda x: re.sub(r\" +\", \" \", x))  # Removing multiple-whitespaces\n",
    "        .str.strip()  # Removing leading and whitepaces\n",
    "    )\n",
    "\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d42f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fasttext_raw = df_raw[[\"case_outcome\", \"case_text\"]]\n",
    "df_fasttext_raw[\"label\"] = \"__label__\" + df_fasttext_raw.case_outcome.str.replace(\" \", \"_\")\n",
    "df_fasttext_raw = df_fasttext_raw[[\"label\", \"case_text\"]]\n",
    "df_fasttext_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa545731",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH_LOCAL = path.cwd().parent.parent.joinpath(\"data\")\n",
    "\n",
    "if not os.path.exists(DATASET_PATH_LOCAL):\n",
    "    os.makedirs(DATASET_PATH_LOCAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab66b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH_LOCAL_FASTTEXT = DATASET_PATH_LOCAL.joinpath(\"fasttext\")\n",
    "\n",
    "if not os.path.exists(DATASET_PATH_LOCAL_FASTTEXT):\n",
    "    os.makedirs(DATASET_PATH_LOCAL_FASTTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcc2b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_CSV_KWARGS = {\n",
    "    \"sep\": \" \",\n",
    "    \"header\": False,\n",
    "    \"index\": False,\n",
    "    \"quoting\": csv.QUOTE_NONE,\n",
    "    \"quotechar\": \"\",\n",
    "    \"escapechar\": \" \",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4660bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fasttext_raw.to_csv(DATASET_PATH_LOCAL_FASTTEXT.joinpath(\"raw.txt\"), **TO_CSV_KWARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87263afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -5 $DATASET_PATH_LOCAL_FASTTEXT\"/raw.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa701647",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = clean_text(df_fasttext_raw, \"case_text\")\n",
    "df_processed.drop_duplicates(subset=\"case_text\", inplace=True)\n",
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b41093",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.to_csv(DATASET_PATH_LOCAL_FASTTEXT.joinpath(\"processed.txt\"), **TO_CSV_KWARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fbc484",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -5 $DATASET_PATH_LOCAL_FASTTEXT\"/processed.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de51a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_processed[\"case_text\"]\n",
    "y = df_processed[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4729c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_, y_train, y_ = train_test_split(X, y, stratify=y, train_size=0.7)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_, y_, stratify=y_, train_size=0.5)\n",
    "\n",
    "print(f\"Training size: {X_train.shape}\")\n",
    "print(f\"Validation size: {X_valid.shape}\")\n",
    "print(f\"Test size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9304213",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(data=[y_train, X_train]).T\n",
    "df_valid = pd.DataFrame(data=[y_valid, X_valid]).T\n",
    "df_test = pd.DataFrame(data=[y_test, X_test]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6451783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(DATASET_PATH_LOCAL_FASTTEXT.joinpath(\"train.txt\"), **TO_CSV_KWARGS)\n",
    "df_valid.to_csv(DATASET_PATH_LOCAL_FASTTEXT.joinpath(\"valid.txt\"), **TO_CSV_KWARGS)\n",
    "df_test.to_csv(DATASET_PATH_LOCAL_FASTTEXT.joinpath(\"test.txt\"), **TO_CSV_KWARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c15518",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (Neptune) Initialize an optuna study-level run\n",
    "**A run is a namespace inside a project where you log metadata.** Typically, you create a run every time you execute a script that does model training, re-training, or inference.  \n",
    "[Read the docs](https://docs.neptune.ai/you-should-know/core-concepts#run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c38532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = neptune.init_run(\n",
    "    project=f\"{WORKSPACE_NAME}/{PROJECT_NAME}\",\n",
    "    name=\"Fasttext text classification\",\n",
    "    description=\"Optuna tuned fasttext text classification\",\n",
    "    tags=[\"fasttext\", \"optuna\", \"study-level\", \"notebook\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70335b51",
   "metadata": {},
   "source": [
    "### (Neptune) Track run-specific files\n",
    "These are the files which are created during the run, and should be tracked within the run and not the project.  \n",
    "[Read the docs](https://docs.neptune.ai/how-to-guides/data-versioning/compare-datasets#step-2-add-tracking-of-the-dataset-version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd2d3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"data/files\"].track_files(os.path.relpath(DATASET_PATH_LOCAL_FASTTEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e1b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_buffer = StringIO()\n",
    "\n",
    "df_fasttext_raw.sample(100).to_csv(csv_buffer, index=False)\n",
    "run[\"data/sample\"].upload(File.from_stream(csv_buffer, extension=\"csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde96727",
   "metadata": {},
   "source": [
    "### (Neptune) Log metadata to run\n",
    "You can log nested dictionaries to create custom nested namespaces.  \n",
    "[Read the docs](https://docs.neptune.ai/you-should-know/logging-metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199db4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"train_size\": len(df_train),\n",
    "    \"test_size\": len(df_test),\n",
    "}\n",
    "\n",
    "run[\"data/metadata\"] = metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dad9dec",
   "metadata": {},
   "source": [
    "### (Neptune) Log sweep and trial parameters\n",
    "Neptune's Optuna integration lets you log metadata from both the study-level and trial-level runs.  \n",
    "[Read the docs](https://docs.neptune.ai/integrations-and-supported-tools/hyperparameter-optimization/optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d8e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "sweep_id = uuid.uuid1()\n",
    "print(f\"Optuna sweep-id: {sweep_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348b16b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"study/sweep_id\"] = sweep_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2350e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_with_logging(trial: optuna.trial.Trial) -> int:\n",
    "    \"\"\"Optuna objective function with inbuilt Neptune tracking\n",
    "\n",
    "    Args:\n",
    "        trial (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"lr\": trial.suggest_float(\"lr\", 0.1, 1, step=0.1),\n",
    "        \"dim\": trial.suggest_int(\"dim\", 10, 1000, log=True),\n",
    "        \"ws\": trial.suggest_int(\"ws\", 1, 10),\n",
    "        \"epoch\": trial.suggest_int(\"epoch\", 10, 100),\n",
    "        \"minCount\": trial.suggest_int(\"minCount\", 1, 10),\n",
    "        \"wordNgrams\": trial.suggest_int(\"wordNgrams\", 1, 3),\n",
    "        \"loss\": trial.suggest_categorical(\"loss\", [\"hs\", \"softmax\", \"ova\"]),\n",
    "        \"bucket\": trial.suggest_int(\"bucket\", 1000000, 3000000, log=True),\n",
    "        \"lrUpdateRate\": trial.suggest_int(\"lrUpdateRate\", 1, 10),\n",
    "        \"t\": trial.suggest_float(\"t\", 0.00001, 0.1, log=True),\n",
    "    }\n",
    "\n",
    "    # (neptune) create a trial-level Run\n",
    "    run_trial_level = neptune.init_run(\n",
    "        project=f\"{WORKSPACE_NAME}/{PROJECT_NAME}\",\n",
    "        name=\"Fasttext text classification\",\n",
    "        description=\"Optuna tuned fasttext text classification\",\n",
    "        tags=[\"fasttext\", \"optuna\", \"trial-level\", \"notebook\"],\n",
    "    )\n",
    "\n",
    "    # (neptune) log sweep id to trial-level Run\n",
    "    run_trial_level[\"sweep_id\"] = sweep_id\n",
    "\n",
    "    # train model on selected hyperparams\n",
    "    clf = fasttext.train_supervised(\n",
    "        input=str(DATASET_PATH_LOCAL_FASTTEXT.joinpath(\"train.txt\")),\n",
    "        verbose=0,\n",
    "        **params,\n",
    "    )\n",
    "\n",
    "    # (neptune) log parameters of a trial-level Run\n",
    "    properties = {k: v for k, v in vars(clf).items() if k not in [\"_words\", \"f\"]}\n",
    "    run_trial_level[\"model/properties\"] = properties\n",
    "\n",
    "    # make predictions and calculate metrics\n",
    "    preds = [clf.predict(text)[0][0] for text in X_valid.values]\n",
    "\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
    "        y_valid,\n",
    "        preds,\n",
    "        average=\"weighted\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    # (neptune) run training and calculate the score for this parameter configuration\n",
    "    run_trial_level[\"validation/metrics/precision\"] = precision\n",
    "    run_trial_level[\"validation/metrics/recall\"] = recall\n",
    "    run_trial_level[\"validation/metrics/f1_score\"] = f1_score\n",
    "\n",
    "    # (neptune) stop trial-level Run\n",
    "    run_trial_level.stop()\n",
    "\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013b5c40",
   "metadata": {},
   "source": [
    "### (Neptune) Create the Neptune callback for Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c180ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune.new.integrations.optuna as optuna_utils\n",
    "\n",
    "neptune_callback = optuna_utils.NeptuneCallback(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe5659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(\n",
    "    objective_with_logging,\n",
    "    n_trials=N_TRIALS,\n",
    "    callbacks=[neptune_callback],\n",
    "    n_jobs=N_JOBS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912ac42c",
   "metadata": {},
   "source": [
    "### (Neptune) Register a model \n",
    "With Neptune's model registry, you can store your ML models in a central location and collaboratively manage their lifecycle.  \n",
    "[Read the docs](https://docs.neptune.ai/how-to-guides/model-registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neptune.init_model(\n",
    "    model=\"TXTCLF-FTXT\",  # Reinitializing an existing model\n",
    "    # name=\"fasttext\", # Required only for new models\n",
    "    # key=\"FTXT\", # Required only for new models\n",
    "    project=f\"{WORKSPACE_NAME}/{PROJECT_NAME}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ccdca6",
   "metadata": {},
   "source": [
    "#### (Neptune) Create a new model version\n",
    "For each model, you can create different versions as you refine the model.  \n",
    "[Read the docs](https://docs.neptune.ai/how-to-guides/model-registry/creating-model-versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec8e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = neptune.init_model_version(\n",
    "    project=f\"{WORKSPACE_NAME}/{PROJECT_NAME}\",\n",
    "    model=model.get_structure()[\"sys\"][\"id\"].fetch(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06174ccb",
   "metadata": {},
   "source": [
    "#### (Neptune) Associate model version to run and vice-versa\n",
    "This is to help find the model created by the run in the runs table, and the run which created the model in the models table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dict = {\n",
    "    \"id\": run.get_structure()[\"sys\"][\"id\"].fetch(),\n",
    "    \"name\": run.get_structure()[\"sys\"][\"name\"].fetch(),\n",
    "    \"url\": run.get_run_url(),\n",
    "}\n",
    "run_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5377dc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version[\"run\"] = run_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acf83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version_dict = {\n",
    "    \"id\": model_version.get_structure()[\"sys\"][\"id\"].fetch(),\n",
    "    \"url\": model_version.get_url(),\n",
    "}\n",
    "model_version_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71365f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"model\"] = model_version_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136c40df",
   "metadata": {},
   "source": [
    "#### Train model based on Hyperparameters chosen by Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87838ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = fasttext.train_supervised(\n",
    "    input=str(DATASET_PATH_LOCAL_FASTTEXT.joinpath(\"train.txt\")),\n",
    "    verbose=5,\n",
    "    **study.best_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe94c883",
   "metadata": {},
   "source": [
    "#### (Neptune) Upload serialized model to model registry\n",
    "Similar to artifact tracking in a project/run, you can also track a pointer to the model in the model registry, or upload the entire serialized model object as well.  \n",
    "[Read the docs](https://docs.neptune.ai/how-to-guides/model-registry/creating-model-versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dadd34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = path.cwd().parent.parent.joinpath(\"models\")\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aa4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "MODEL_NAME = str(MODEL_PATH.joinpath(f\"fasttext_{datetime.now().strftime('%Y%m%d%H%M%S')}.bin\"))\n",
    "clf.save_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbcb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.getsize(MODEL_NAME) < 1024 * 1024 * UPLOAD_SIZE_THRESHOLD:\n",
    "    print(\"Uploading serialized model\")\n",
    "    model_version[\"serialized_model\"].upload(str(MODEL_NAME))\n",
    "else:\n",
    "    print(f\"Model is larger than UPLOAD_SIZE_THRESHOLD ({UPLOAD_SIZE_THRESHOLD} MB). Tracking pointer to model file\")\n",
    "    model_version[\"serialized_model\"].track_files(os.path.relpath(MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe76d24",
   "metadata": {},
   "source": [
    "#### (Neptune) Log model properties to model_version\n",
    "Neptune dynamically creates nested namespaces based on the dictionary structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = {k: v for k, v in vars(clf).items() if k not in [\"_words\", \"f\"]}\n",
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb2c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version[\"properties\"] = properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e4c84a",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e43365",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [clf.predict(text)[0][0] for text in X_test.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"prediction\"] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e307417b",
   "metadata": {},
   "source": [
    "### (Neptune) Log parameters, metrics and debugging information to run and model version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68088aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
    "    y_test,\n",
    "    preds,\n",
    "    average=\"weighted\",\n",
    "    zero_division=0,\n",
    ")\n",
    "\n",
    "print(f\"Precision: {precision}\\nRecall: {recall}\\nF1-score: {f1_score}\")\n",
    "\n",
    "run[\"test/metrics/precision\"] = model_version[\"metrics/precision\"] = precision\n",
    "run[\"test/metrics/recall\"] = model_version[\"metrics/recall\"] = recall\n",
    "run[\"test/metrics/f1_score\"] = model_version[\"metrics/f1_score\"] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (neptune) Log each metric in its separate nested namespace\n",
    "print(json.dumps(classification_report(y_test, preds, output_dict=True, zero_division=0), indent=2))\n",
    "run[\"test/metrics/classification_report\"] = classification_report(y_test, preds, output_dict=True, zero_division=0)\n",
    "\n",
    "# (neptune) Log classification report as an HTML dataframe\n",
    "df_clf_rpt = pd.DataFrame(classification_report(y_test, preds, output_dict=True, zero_division=0)).T\n",
    "run[\"test/metrics/classification_report/report\"].upload(File.as_html(df_clf_rpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b07fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ConfusionMatrixDisplay.from_predictions(y_test, preds, xticks_rotation=\"vertical\", colorbar=False)\n",
    "run[\"test/debug/plots/confusion_matrix\"].upload(fig.figure_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a10c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [s.replace(\"__label__\", \"\") for s in df_test.label.value_counts().index]\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Bar(name=\"Actual\", x=labels, y=df_test.label.value_counts()),\n",
    "        go.Bar(name=\"Prediction\", x=labels, y=df_test.prediction.value_counts()),\n",
    "    ]\n",
    ")\n",
    "fig.update_layout(title=\"Actual vs Prediction\", barmode=\"group\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d807cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"test/debug/plots/prediction_distribution\"].upload(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4ef5d6",
   "metadata": {},
   "source": [
    "### (Neptune) Log misclassified results\n",
    "CSV files logged to Neptune will be rendered as an interactive table.  \n",
    "[Read the docs](https://docs.neptune.ai/you-should-know/what-can-you-log-and-display#csv-files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f01c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_debug = df_test[df_test.label != df_test.prediction]\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "\n",
    "df_debug.to_csv(csv_buffer, index=False)\n",
    "run[\"test/debug/misclassifications\"].upload(File.from_stream(csv_buffer, extension=\"csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c9370",
   "metadata": {},
   "source": [
    "### (Neptune) Stop model version, model, run, and project\n",
    "When a script finishes executing, runs are stopped automatically. However, to avoid logging metadata for longer than intended, it's best to stop the run explicitly with the `stop()` method.  \n",
    "When working in an interactive notebook environment, you should always call `stop()`. This is because the connection to Neptune is not stopped when the cell has finished executing, but rather only when you stop the notebook.  \n",
    "[Read the docs](https://docs.neptune.ai/api-reference/project#.stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version.stop()\n",
    "model.stop()\n",
    "run.stop()\n",
    "project.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d56c83",
   "metadata": {},
   "source": [
    "## (Neptune) Explore the [project](https://app.neptune.ai/showcase/project-text-classification) in the Neptune app\n",
    "https://app.neptune.ai/showcase/project-text-classification"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "neptune": {
   "notebookId": "9732b722-0ff0-4d32-87ae-51ffcc75ab45",
   "projectVersion": 2
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "vscode": {
   "interpreter": {
    "hash": "ec001e92a202bf5caf822e845ea5ad1be0f2e89477ccf084fef05c1d70e5b02a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
